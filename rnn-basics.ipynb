{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß† Recurrent Neural Network (RNN) - Imports & Basic Setup (Explanation Only)\n\n## üì¶ Step 1: Importing Libraries\n\n### üß© TensorFlow\nTensorFlow is the core deep learning framework that provides all the building blocks for neural networks.  \nIt allows us to build, train, and evaluate complex models efficiently using both CPU and GPU.\n\n---\n\n### üß© NumPy\nNumPy is a Python library for handling numerical data.  \nIt is used to create arrays, reshape data, and perform mathematical operations before feeding data into the model.\n\n---\n\n### üß© Keras Sequential Model\nSequential is a **linear stack** of layers.  \nIt means you build your model **step-by-step** ‚Äî from input to output ‚Äî in a straight path (no branching or complex graphs).  \nUsed for most beginner and medium-level deep learning tasks.\n\n---\n\n### üß© SimpleRNN Layer\nThe SimpleRNN layer is the **basic Recurrent Neural Network** layer.  \nIt reads input **sequence by sequence** and remembers information from previous time steps using an internal memory (hidden state).  \nIt‚Äôs best suited for simple sequence prediction tasks like:\n- Predicting next number in a series  \n- Time-series or stock predictions  \n- Simple language modeling  \n\n---\n\n### üß© Dense Layer\nA Dense layer is a **fully connected layer**, where each neuron receives input from **every neuron** in the previous layer.  \nIt‚Äôs typically used at the **output stage** of the model for making the final prediction.  \nExamples:\n- 1 neuron ‚Üí for regression or numeric prediction  \n- Multiple neurons ‚Üí for classification (softmax, sigmoid, etc.)\n\n---\n\n## üß† Summary Table\n\n| Component | Role in Model | Key Idea |\n|------------|----------------|-----------|\n| **TensorFlow** | Core deep learning library | Provides all tools for building neural nets |\n| **NumPy** | Data handling and math | Prepares input data as arrays |\n| **Sequential** | Model structure | Builds model layer by layer |\n| **SimpleRNN** | Processes sequential data | Remembers past information |\n| **Dense** | Final prediction layer | Converts learned pattern into output |\n\n---\n\n## ‚öôÔ∏è Workflow Reminder\nThe standard RNN process:\n1. **Import libraries**  \n2. **Prepare input data** (convert to NumPy arrays, normalize, reshape to 3D)\n3. **Build model** using Sequential + SimpleRNN + Dense layers  \n4. **Compile** model (choose optimizer and loss)\n5. **Train** with `.fit()`\n6. **Predict** with `.predict()`\n\n---\n\n## üí° Key Insights\n- RNNs are powerful for **time-dependent or sequential data**.  \n- Always **reshape data to (samples, timesteps, features)** for RNNs.  \n- **Normalization** (scaling data to smaller ranges like 0‚Äì1) prevents unstable training.  \n- Start simple (`SimpleRNN`), then move to advanced versions like **LSTM** or **GRU** later.\n\n---\n\nüìö **Keywords for Revision:**  \nTensorFlow, Keras, Sequential, SimpleRNN, Dense, Sequential Data, Hidden State, Time Steps, Normalization\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import SimpleRNN ,Dense\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìä Data Preparation and Reshaping for RNN\n\n## üß© Step 1: Creating Input and Output Data\nWe first define our input (`X`) and target (`Y`) data using numerical sequences.  \nFor example, if the goal is to predict the **next number in a sequence**, we give the RNN sequences like:\n- Input: `[10, 20, 30, 40]` ‚Üí Output: `50`\n- Input: `[20, 30, 40, 50]` ‚Üí Output: `60`\n\nThis helps the RNN learn the **pattern** or **trend** between numbers.\n\n---\n\n## üß© Step 2: Understanding the Shape of RNN Input\n\nRNNs expect input in **three dimensions**:\n\n| Dimension | Meaning | Example |\n|------------|----------|---------|\n| **Samples** | How many sequences we have | 4 sequences |\n| **Time Steps** | How many elements in one sequence | 4 numbers in each |\n| **Features** | How many variables at each time step | 1 (only one value per time step) |\n\nSo the input shape should be:  \n`(samples, time_steps, features)` = `(4, 4, 1)`\n\nWe reshape our data to match this format so that the RNN understands the sequential structure correctly.\n\n---\n\n## üß© Step 3: Normalization (Scaling Data)\nNormalization means bringing all numbers into a small range (usually between **0 and 1**).  \nThis step prevents large numbers from causing unstable gradients during training.\n\n| Before Normalization | After Normalization (√∑100) |\n|----------------------|-----------------------------|\n| 10 ‚Üí 80 | 0.10 ‚Üí 0.80 |\n\nBenefits of Normalization:\n- Makes model training **faster and more stable**\n- Prevents the **\"overshoot\"** problem (where predictions become too high or too low)\n- Keeps values consistent across all features\n\n---\n\n## üß© Step 4: Output Normalization\nWe also scale the output (`Y`) using the same factor so that:\n- Model learns relationships in the same scale as inputs.\n- Predictions can be rescaled back to the original range by multiplying by the same factor (e.g., √ó100).\n\n---\n\n## üß† Why All This Matters\nRNNs work best when:\n1. Data has a **clear sequential order**  \n2. Input is **reshaped properly** (3D)  \n3. All features are **normalized** to small values  \n\nIf any of these are skipped, the model may fail to learn the true pattern or predict values incorrectly.\n\n---\n\n## üí° Quick Recap\n| Step | Purpose |\n|------|----------|\n| Create input & output arrays | Define the pattern the model will learn |\n| Reshape data | Match RNN‚Äôs 3D input format |\n| Normalize input & output | Stabilize and speed up training |\n\n---\n\nüìö **Keywords for Revision:**  \nReshaping, Normalization, Time Steps, Samples, Features, Sequential Data, Scaling, Gradient Stability\n","metadata":{}},{"cell_type":"code","source":"x=np.array([[10,20,30,40],[20,30,40,50],[30,40,50,60],[40,50,60,70]],dtype=float)\ny=np.array([50,60,70,80],dtype=float)\nx=x.reshape((x.shape[0],x.shape[1],1))\nx=x/100\ny=y/100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T12:59:45.346963Z","iopub.execute_input":"2025-10-07T12:59:45.347607Z","iopub.status.idle":"2025-10-07T12:59:45.367489Z","shell.execute_reply.started":"2025-10-07T12:59:45.347579Z","shell.execute_reply":"2025-10-07T12:59:45.366671Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# üèóÔ∏è Building the RNN Model\n\n## üß© Step 1: Sequential Model\nThe **Sequential model** is a simple linear stack of layers.  \nIt allows us to build a network step-by-step ‚Äî input ‚Üí hidden ‚Üí output.\n\n---\n\n## üß© Step 2: SimpleRNN Layer\n- The **SimpleRNN layer** is the core of the model.  \n- It processes input sequences one step at a time and keeps a short-term memory of previous steps.  \n- The parameter `(50)` means **50 neurons**, giving the model more capacity to learn complex patterns.  \n- Using the **`tanh` activation** helps the RNN handle both positive and negative values smoothly and avoid exploding outputs.  \n- The **`input_shape=(4,1)`** tells the network each input sequence has 4 time steps and 1 feature per step.\n\n---\n\n## üß© Step 3: Dense Layer\n- The **Dense layer** is the final output layer.  \n- It connects every neuron from the RNN to a single output value.  \n- Perfect for predicting a single number (like the next value in a series).\n\n---\n\n## ‚öôÔ∏è Summary\n| Layer | Role | Key Idea |\n|--------|------|----------|\n| **SimpleRNN(50, tanh)** | Learns pattern from sequence | 50 memory units, nonlinear activation |\n| **Dense(1)** | Outputs the prediction | Converts learned pattern to one numeric value |\n\n---\n\n## üí° Takeaway\nThis is a **minimal yet powerful RNN structure** for sequence prediction:  \n- Simple enough for beginners  \n- Strong enough to capture numeric trends  \n- Easy to expand later (add Dropout, LSTM, GRU, etc.)\n","metadata":{}},{"cell_type":"code","source":"model=Sequential()\nmodel.add(SimpleRNN(50,activation='tanh',input_shape=(4,1)))\nmodel.add(Dense(1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T12:59:45.368440Z","iopub.execute_input":"2025-10-07T12:59:45.368691Z","iopub.status.idle":"2025-10-07T12:59:45.415102Z","shell.execute_reply.started":"2025-10-07T12:59:45.368672Z","shell.execute_reply":"2025-10-07T12:59:45.414432Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# ‚öôÔ∏è Model Compilation & Training\n\n## üß© Model Compilation\n- **Optimizer:** `adam` ‚Äî an adaptive optimizer that adjusts learning rate automatically for faster convergence.  \n- **Loss:** `mse` (Mean Squared Error) ‚Äî measures how far predictions are from real values.  \n  Lower MSE ‚Üí better learning.\n\n---\n\n## üß© Model Training\n- **`.fit(x, y, epochs=100)`** trains the model for 100 passes over the data.  \n- Each epoch helps the RNN learn the sequence pattern more accurately.  \n- **`verbose=0`** hides training logs for cleaner output.\n\n---\n\n## üí° In Short\nCompilation sets **how the model learns** (optimizer & loss).  \nTraining (`fit`) is **when the model actually learns** from data.\n","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam',\n             loss='mse')\nmodel.fit(x,y,epochs=100,verbose=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T12:59:45.415982Z","iopub.execute_input":"2025-10-07T12:59:45.416286Z","iopub.status.idle":"2025-10-07T12:59:51.424360Z","shell.execute_reply.started":"2025-10-07T12:59:45.416247Z","shell.execute_reply":"2025-10-07T12:59:51.423679Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7b8b95eaab10>"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"# üéØ Making Predictions\n\n## üß© Preparing Test Data\n- The input sequence is reshaped to match RNN‚Äôs expected shape: `(samples, timesteps, features)`.  \n- **Normalization** is applied (divide by 100) to keep data in the same scale as training.\n\n---\n\n## üß© Predicting the Output\n- **`model.predict()`** feeds the test sequence into the trained RNN.  \n- The RNN uses its learned memory to predict the **next value in the sequence**.\n\n---\n\n## üß© Interpreting Results\n- The raw prediction is scaled back to the original range by multiplying by 100.  \n- **Formatting** (e.g., `.1f`) rounds the number for easier reading.  \n\nExample:  \n- Input: `[50, 60, 70, 80]`  \n- Predicted Output: `‚âà 90`  \n\n---\n\n## üí° Key Insight\nPrediction confirms if the RNN **understood the sequence pattern**.  \n- Correct scaling and reshaping are crucial for accurate predictions.  \n- Even with a tiny dataset, a simple RNN can capture linear trends.\n","metadata":{}},{"cell_type":"code","source":"test=np.array([50,60,70,80]).reshape((1,4,1))/100\npred=model.predict(test)\nprint(f'predicted value:{pred[0][0]*100:.1f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T13:03:36.253790Z","iopub.execute_input":"2025-10-07T13:03:36.254112Z","iopub.status.idle":"2025-10-07T13:03:36.353752Z","shell.execute_reply.started":"2025-10-07T13:03:36.254089Z","shell.execute_reply":"2025-10-07T13:03:36.352856Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\npredicted value:90.6\n","output_type":"stream"}],"execution_count":40}]}