{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ§  LSTM Basics \n\n## ğŸ“¦ Data Preparation\n- Inputs (`x`) are short numeric sequences like `[1,2,3]`, `[2,3,4]` â€” representing time steps.\n- Targets (`y`) are the **next numbers** in each sequence.\n- Data reshaped to `(samples, timesteps, features)` for LSTM input.\n- Normalized (divided by 100) for stable and faster training.\n\n---\n\n## âš™ï¸ LSTM Model\n- **LSTM Layer:** Learns patterns over time â€” better than RNN at remembering longer dependencies.\n- **Dense Layer:** Outputs the next predicted number.\n\n---\n\n## ğŸ“š Key Idea\nLSTM looks at previous time steps and predicts the **next value** in the sequence.  \nItâ€™s like teaching the model to â€œguess the next numberâ€ based on recent memory.\n\n---\n\n## ğŸ’¡ Why LSTM?\n- Handles longer sequences better than SimpleRNN.  \n- Remembers important patterns, forgets noise.  \n- Widely used in **stock prediction, language models, and sensor data**.\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM , Dense\n\nx=np.array([[1,2,3],[2,3,4],[3,4,5],[4,5,6]],dtype=float)\ny=np.array([4,5,6,7],dtype=float)\nx=x.reshape(x.shape[0],x.shape[1],1)\nx=x/100\ny=y/100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:49:56.373469Z","iopub.execute_input":"2025-10-07T14:49:56.373849Z","iopub.status.idle":"2025-10-07T14:49:56.381001Z","shell.execute_reply.started":"2025-10-07T14:49:56.373825Z","shell.execute_reply":"2025-10-07T14:49:56.380085Z"}},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":"# ğŸ—ï¸ LSTM Model Building\n\n## ğŸ§© Sequential Model\n- A **linear stack of layers**: input â†’ LSTM â†’ Dense.\n- Simple and beginner-friendly.\n\n---\n\n## ğŸ§© LSTM Layer\n- **30 neurons** â†’ more capacity to capture patterns.\n- **`tanh` activation** â†’ handles positive & negative values smoothly.\n- **Input shape `(3,1)`** â†’ 3 time steps, 1 feature per step.\n\n---\n\n## ğŸ§© Dense Layer\n- Outputs a single number (the next value in the sequence).\n- Fully connected to the LSTM output.\n\n---\n\n## ğŸ’¡ Key Insight\nThis is a minimal yet effective LSTM setup for **sequence prediction**:  \n- Stronger memory than SimpleRNN  \n- Handles small numeric sequences perfectly  \n- Easy to scale later (more neurons, more layers, or dropout)\n","metadata":{}},{"cell_type":"code","source":"model=Sequential()\nmodel.add(LSTM(30,activation='tanh',input_shape=(3,1)))\nmodel.add(Dense(1))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:49:56.382845Z","iopub.execute_input":"2025-10-07T14:49:56.383464Z","iopub.status.idle":"2025-10-07T14:49:56.426156Z","shell.execute_reply.started":"2025-10-07T14:49:56.383439Z","shell.execute_reply":"2025-10-07T14:49:56.425321Z"}},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":"# âš™ï¸ LSTM Model Compilation\n\n## ğŸ§© Optimizer\n- **`adam`**: Adaptive optimizer that adjusts learning rates automatically.  \n- Helps the model learn faster and more efficiently.\n\n## ğŸ§© Loss Function\n- **`mse` (Mean Squared Error)**: Measures the difference between predicted and actual values.  \n- Lower MSE â†’ better prediction accuracy.\n\n## ğŸ’¡ Key Insight\n- Compilation **does not train the model**; it just sets **how the model will learn**.  \n- Choosing the right optimizer and loss is crucial for stable training and convergence.\n","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam',loss='mse')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:49:56.427030Z","iopub.execute_input":"2025-10-07T14:49:56.427309Z","iopub.status.idle":"2025-10-07T14:49:56.437201Z","shell.execute_reply.started":"2025-10-07T14:49:56.427282Z","shell.execute_reply":"2025-10-07T14:49:56.436312Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"# ğŸ‹ï¸ Training the LSTM Model\n\n## ğŸ§© Training Process\n- **`.fit(x, y, epochs=500)`**: Trains the model on the input-output data for 500 passes (epochs).  \n- Each epoch helps the model **adjust weights** to better predict the next number in the sequence.\n\n## ğŸ§© Verbose\n- **`verbose=0`**: Hides the training logs for a cleaner output.  \n\n## ğŸ’¡ Key Insight\n- Training is when the model actually **learns the patterns** from the data.  \n- More epochs allow the model to better capture trends, especially for small datasets.  \n- LSTM remembers previous time steps, so even short sequences are learned efficiently.\n","metadata":{}},{"cell_type":"code","source":"model.fit(x,y,epochs=500,verbose=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:49:56.438153Z","iopub.execute_input":"2025-10-07T14:49:56.438404Z","iopub.status.idle":"2025-10-07T14:50:21.086278Z","shell.execute_reply.started":"2025-10-07T14:49:56.438385Z","shell.execute_reply":"2025-10-07T14:50:21.085402Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x78a30d7076d0>"},"metadata":{}}],"execution_count":57},{"cell_type":"markdown","source":"# ğŸ¯ LSTM Prediction\n\n## ğŸ§© Preparing Test Data\n- Reshape the input to match LSTMâ€™s expected shape: `(samples, timesteps, features)`  \n- Normalize the test input the same way as training data (divide by 100).  \n\n## ğŸ§© Making Predictions\n- **`model.predict()`**: Feeds the test sequence into the trained LSTM.  \n- The LSTM uses its learned memory to predict the **next number** in the sequence.\n\n## ğŸ§© Interpreting Results\n- Multiply by 100 to rescale the prediction to the original range.  \n- Use formatting (e.g., `.2f`) for a clean, readable output.\n\n### ğŸ’¡ Key Insight\n- Prediction confirms whether the LSTM **understood the sequence pattern**.  \n- Proper reshaping and normalization are critical for accurate results.  \n- Even small sequences can give accurate predictions when the model has learned the trend.\n","metadata":{}},{"cell_type":"code","source":"test=np.array([5,6,7]).reshape((1,3,1))/100\npred=model.predict(test)\nprint(f'prediction:{pred[0][0]*100:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:50:21.088019Z","iopub.execute_input":"2025-10-07T14:50:21.088341Z","iopub.status.idle":"2025-10-07T14:50:21.396556Z","shell.execute_reply.started":"2025-10-07T14:50:21.088318Z","shell.execute_reply":"2025-10-07T14:50:21.395591Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\nprediction:8.01\n","output_type":"stream"}],"execution_count":58}]}